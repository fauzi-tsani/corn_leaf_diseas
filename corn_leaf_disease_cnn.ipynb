{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T08:35:48.102953Z",
     "start_time": "2025-11-09T08:35:45.276032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ],
   "id": "6259bf10265eba5a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T08:35:49.464315Z",
     "start_time": "2025-11-09T08:35:49.371257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define image dimensions and paths\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "BATCH_SIZE = 32\n",
    "DATA_DIR = 'C:/Users/fauzi/PycharmProjects/Corn Leaf Disease/dataset/train'\n",
    "\n",
    "# --- Data Processing Framework ---\n",
    "# We will split the data from the train directory into training and validation sets.\n",
    "# Data Augmentation and Rescaling for Training Data, also specifying the validation split.\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # Splitting 20% of the data for validation\n",
    ")\n",
    "\n",
    "# Flow training images in batches from directory\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Specify this is the training set\n",
    ")\n",
    "\n",
    "# Flow validation images in batches from directory\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',  # Specify this is the validation set\n",
    "    shuffle=False # Important for evaluation\n",
    ")\n",
    "\n",
    "# Print class indices to verify\n",
    "print(\"Class indices:\", train_generator.class_indices)\n",
    "NUM_CLASSES = len(train_generator.class_indices)\n"
   ],
   "id": "6109cd5179db5bba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4328 images belonging to 3 classes.\n",
      "Found 1080 images belonging to 3 classes.\n",
      "Class indices: {'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot': 0, 'Corn_(maize)___Common_rust_': 1, 'Corn_(maize)___healthy': 2}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Explanation of the Data Preprocessing Method\n",
    "\n",
    "The method used for data preprocessing is a standard and highly effective approach for image classification tasks, especially when working with deep learning models like Convolutional Neural Networks (CNNs). Here's a breakdown of why this method is so effective:\n",
    "\n",
    "**1. `ImageDataGenerator`:**\n",
    "\n",
    "*   **Efficiency:** Instead of loading all the images into memory at once (which can be very memory-intensive, especially with large datasets), `ImageDataGenerator` creates a Python generator that loads the images in batches. This is far more memory-efficient and is a common practice in deep learning.\n",
    "*   **On-the-Fly Augmentation:** The generator applies data augmentation transformations to the images as they are being loaded. This means that the model sees slightly different versions of the same image in each epoch, which helps to improve the model's ability to generalize.\n",
    "*   **Validation Split:** The `validation_split` argument is a convenient way to automatically reserve a portion of your training data for validation. This is crucial for monitoring the model's performance on unseen data during training.\n",
    "\n",
    "**2. Data Augmentation:**\n",
    "\n",
    "*   **Reduces Overfitting:** Data augmentation artificially expands the training dataset by creating modified versions of the images. This helps to prevent the model from \"memorizing\" the training data and improves its ability to generalize to new, unseen images.\n",
    "\n",
    "**3. `flow_from_directory`:**\n",
    "\n",
    "*   **Convenience:** This function is incredibly convenient. As long as your directory structure is set up correctly (with separate subdirectories for each class), `flow_from_directory` will automatically infer the class labels from the directory names.\n",
    "*   **Batching and Resizing:** It handles the batching of the data (controlled by `batch_size`) and resizes the images to the desired `target_size` on the fly.\n",
    "\n",
    "In summary, this data preprocessing pipeline is a robust and efficient way to prepare image data for training a deep learning model. It handles memory management, data augmentation, and the creation of data batches, all of which are essential for successful model training.\n"
   ],
   "id": "acc138064a2403b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- VGG-16 Model Architecture ---\n",
    "\n",
    "# Load the VGG-16 model, pre-trained on ImageNet, without the top classification layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "\n",
    "# Freeze the convolutional base\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model on top\n",
    "vgg_model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "vgg_model.summary()\n",
    "\n",
    "# Compile the model\n",
    "vgg_model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n"
   ],
   "id": "934c42475df259e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Train the Model ---\n",
    "history = vgg_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(train_generator.samples / BATCH_SIZE),\n",
    "    epochs=10, # You can increase the number of epochs\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=np.ceil(validation_generator.samples / BATCH_SIZE)\n",
    ")\n"
   ],
   "id": "67c603a6de64cc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Plot Training and Validation Accuracy/Loss ---\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ],
   "id": "b3a9a51e7061c0a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Evaluate the Model on the Validation Set ---\n",
    "# Evaluate the model\n",
    "val_loss, val_acc = vgg_model.evaluate(validation_generator)\n",
    "print(f'Validation accuracy: {val_acc}')\n",
    "\n",
    "# Get predictions for the validation set\n",
    "Y_pred = vgg_model.predict(validation_generator)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "# Get true labels\n",
    "y_true = validation_generator.classes\n",
    "\n",
    "# Confusion Matrix\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Classification Report\n",
    "print('Classification Report')\n",
    "target_names = list(train_generator.class_indices.keys())\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n"
   ],
   "id": "a9378c753e6d4dda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Fine-Tuning Strategy\n",
    "\n",
    "My fine-tuning strategy involves a two-step process. Initially, I freeze the pre-trained convolutional base of the VGG-16 model and only train the newly added, randomly initialized classification layers. This allows the new layers to learn the specific features of the corn leaf disease dataset without disrupting the learned representations in the convolutional base. Once the new layers have converged, I will unfreeze some of the top layers of the convolutional base and continue training the entire network with a very low learning rate. This second step allows the model to \"fine-tune\" the pre-trained features to the specific dataset, potentially leading to a further increase in performance. This approach prevents large, random weight updates from destroying the pre-trained weights in the early stages of training and allows for a more gradual and effective adaptation of the model to the new task.\n"
   ],
   "id": "d0b8f2da06a2e8bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
