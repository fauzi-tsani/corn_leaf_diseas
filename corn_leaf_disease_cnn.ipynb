{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T08:48:58.029857Z",
     "start_time": "2025-11-09T08:48:55.343939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ],
   "id": "6259bf10265eba5a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T08:48:58.704742Z",
     "start_time": "2025-11-09T08:48:58.599129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define image dimensions and paths\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "BATCH_SIZE = 32\n",
    "DATA_DIR = 'C:/Users/fauzi/PycharmProjects/Corn Leaf Disease/dataset/train'\n",
    "\n",
    "# --- Data Processing Framework ---\n",
    "# We will split the data from the train directory into training and validation sets.\n",
    "# Data Augmentation and Rescaling for Training Data, also specifying the validation split.\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # Splitting 20% of the data for validation\n",
    ")\n",
    "\n",
    "# Flow training images in batches from directory\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Specify this is the training set\n",
    ")\n",
    "\n",
    "# Flow validation images in batches from directory\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',  # Specify this is the validation set\n",
    "    shuffle=False # Important for evaluation\n",
    ")\n",
    "\n",
    "# Print class indices to verify\n",
    "print(\"Class indices:\", train_generator.class_indices)\n",
    "NUM_CLASSES = len(train_generator.class_indices)\n"
   ],
   "id": "6109cd5179db5bba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4328 images belonging to 3 classes.\n",
      "Found 1080 images belonging to 3 classes.\n",
      "Class indices: {'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot': 0, 'Corn_(maize)___Common_rust_': 1, 'Corn_(maize)___healthy': 2}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Explanation of the Data Preprocessing Method\n",
    "\n",
    "The method used for data preprocessing is a standard and highly effective approach for image classification tasks, especially when working with deep learning models like Convolutional Neural Networks (CNNs). Here's a breakdown of why this method is so effective:\n",
    "\n",
    "**1. `ImageDataGenerator`:**\n",
    "\n",
    "*   **Efficiency:** Instead of loading all the images into memory at once (which can be very memory-intensive, especially with large datasets), `ImageDataGenerator` creates a Python generator that loads the images in batches. This is far more memory-efficient and is a common practice in deep learning.\n",
    "*   **On-the-Fly Augmentation:** The generator applies data augmentation transformations to the images as they are being loaded. This means that the model sees slightly different versions of the same image in each epoch, which helps to improve the model's ability to generalize.\n",
    "*   **Validation Split:** The `validation_split` argument is a convenient way to automatically reserve a portion of your training data for validation. This is crucial for monitoring the model's performance on unseen data during training.\n",
    "\n",
    "**2. Data Augmentation:**\n",
    "\n",
    "*   **Reduces Overfitting:** Data augmentation artificially expands the training dataset by creating modified versions of the images. This helps to prevent the model from \"memorizing\" the training data and improves its ability to generalize to new, unseen images.\n",
    "\n",
    "**3. `flow_from_directory`:**\n",
    "\n",
    "*   **Convenience:** This function is incredibly convenient. As long as your directory structure is set up correctly (with separate subdirectories for each class), `flow_from_directory` will automatically infer the class labels from the directory names.\n",
    "*   **Batching and Resizing:** It handles the batching of the data (controlled by `batch_size`) and resizes the images to the desired `target_size` on the fly.\n",
    "\n",
    "In summary, this data preprocessing pipeline is a robust and efficient way to prepare image data for training a deep learning model. It handles memory management, data augmentation, and the creation of data batches, all of which are essential for successful model training.\n"
   ],
   "id": "acc138064a2403b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T08:49:05.026917Z",
     "start_time": "2025-11-09T08:49:04.742198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- VGG-16 Model Architecture ---\n",
    "\n",
    "# Load the VGG-16 model, pre-trained on ImageNet, without the top classification layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "\n",
    "# Freeze the convolutional base\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model on top\n",
    "vgg_model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "vgg_model.summary()\n",
    "\n",
    "# Compile the model\n",
    "vgg_model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n"
   ],
   "id": "934c42475df259e2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ vgg16 (\u001B[38;5;33mFunctional\u001B[0m)              │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m512\u001B[0m)      │    \u001B[38;5;34m14,714,688\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001B[38;5;33mFlatten\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m25088\u001B[0m)          │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m)            │    \u001B[38;5;34m12,845,568\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001B[38;5;33mDropout\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m3\u001B[0m)              │         \u001B[38;5;34m1,539\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,845,568</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,539</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m27,561,795\u001B[0m (105.14 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,561,795</span> (105.14 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m12,847,107\u001B[0m (49.01 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,847,107</span> (49.01 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m14,714,688\u001B[0m (56.13 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T08:49:24.039718Z",
     "start_time": "2025-11-09T08:49:22.736689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Train the Model ---\n",
    "history = vgg_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(train_generator.samples / BATCH_SIZE),\n",
    "    epochs=10, # You can increase the number of epochs\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=np.ceil(validation_generator.samples / BATCH_SIZE)\n",
    ")\n"
   ],
   "id": "67c603a6de64cc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# --- Train the Model ---\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m history = \u001B[43mvgg_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mceil\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_generator\u001B[49m\u001B[43m.\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m \u001B[49m\u001B[43m/\u001B[49m\u001B[43m \u001B[49m\u001B[43mBATCH_SIZE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# You can increase the number of epochs\u001B[39;49;00m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvalidation_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvalidation_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mceil\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalidation_generator\u001B[49m\u001B[43m.\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m \u001B[49m\u001B[43m/\u001B[49m\u001B[43m \u001B[49m\u001B[43mBATCH_SIZE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Corn Leaf Disease\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    119\u001B[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001B[32m    120\u001B[39m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[32m    121\u001B[39m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m122\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    123\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    124\u001B[39m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Corn Leaf Disease\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:113\u001B[39m, in \u001B[36mEpochIterator._enumerate_iterator\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    111\u001B[39m     \u001B[38;5;28mself\u001B[39m._current_iterator = \u001B[38;5;28miter\u001B[39m(\u001B[38;5;28mself\u001B[39m._get_iterator())\n\u001B[32m    112\u001B[39m     \u001B[38;5;28mself\u001B[39m._steps_seen = \u001B[32m0\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m113\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msteps_per_execution\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m    114\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_batches \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._steps_seen >= \u001B[38;5;28mself\u001B[39m._num_batches:\n\u001B[32m    115\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.steps_per_epoch:\n",
      "\u001B[31mTypeError\u001B[39m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Plot Training and Validation Accuracy/Loss ---\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ],
   "id": "b3a9a51e7061c0a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Evaluate the Model on the Validation Set ---\n",
    "# Evaluate the model\n",
    "val_loss, val_acc = vgg_model.evaluate(validation_generator)\n",
    "print(f'Validation accuracy: {val_acc}')\n",
    "\n",
    "# Get predictions for the validation set\n",
    "Y_pred = vgg_model.predict(validation_generator)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "# Get true labels\n",
    "y_true = validation_generator.classes\n",
    "\n",
    "# Confusion Matrix\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Classification Report\n",
    "print('Classification Report')\n",
    "target_names = list(train_generator.class_indices.keys())\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n"
   ],
   "id": "a9378c753e6d4dda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Fine-Tuning Strategy\n",
    "\n",
    "My fine-tuning strategy involves a two-step process. Initially, I freeze the pre-trained convolutional base of the VGG-16 model and only train the newly added, randomly initialized classification layers. This allows the new layers to learn the specific features of the corn leaf disease dataset without disrupting the learned representations in the convolutional base. Once the new layers have converged, I will unfreeze some of the top layers of the convolutional base and continue training the entire network with a very low learning rate. This second step allows the model to \"fine-tune\" the pre-trained features to the specific dataset, potentially leading to a further increase in performance. This approach prevents large, random weight updates from destroying the pre-trained weights in the early stages of training and allows for a more gradual and effective adaptation of the model to the new task.\n"
   ],
   "id": "d0b8f2da06a2e8bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
